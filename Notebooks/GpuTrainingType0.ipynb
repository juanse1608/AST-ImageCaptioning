{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfJZLpl-iDjs",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Loading-Images-and-Captions\" data-toc-modified-id=\"Loading-Images-and-Captions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loading Images and Captions</a></span></li><li><span><a href=\"#Split:-Train-and-Test\" data-toc-modified-id=\"Split:-Train-and-Test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Split: Train and Test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob7dvTLiLjr8"
   },
   "source": [
    "# Image Captioning - Advanced Statistics Topics\n",
    "\n",
    "In this notebook we adecuate the enviroment to sue the GPU provided by Google Colab. Unfortunately, the storage (CPU) capacity of the vm is too low and we have to read, process, train the model and delete little batches of the training images.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6lCo231DGwz"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "In this section we import the requiere libraries and establish the connection with Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.675664Z",
     "start_time": "2021-04-28T15:12:11.016400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1619731137887,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "WdtmqKdzMA0W",
    "outputId": "8cdd771f-6887-4be6-979a-ca5fbb7d7855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORFLOW VERSION: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Se importan las librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import gzip\n",
    "import timeit\n",
    "import json\n",
    "import collections\n",
    "import zipfile\n",
    "import random\n",
    "import time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "# from google.colab import drive # Necesario para GoogleColab\n",
    "\n",
    "# Tensorflow 2.x\n",
    "import tensorflow as tf # Not required for this notebook \n",
    "print('TENSORFLOW VERSION: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.679133Z",
     "start_time": "2021-04-28T15:12:14.677248Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35290,
     "status": "ok",
     "timestamp": 1619731172086,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "zp8cp8wMMBhN",
    "outputId": "57a31698-4088-4552-ba5a-876f7e37bf37"
   },
   "outputs": [],
   "source": [
    "# Se fija el directorio referencia con el se conecta colab a drive # Necesario para GoogleColab\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.683150Z",
     "start_time": "2021-04-28T15:12:14.681323Z"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1619731174450,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "b9mNBsu2ENCt"
   },
   "outputs": [],
   "source": [
    "# Se guarda en una variable el directorio del curso\n",
    "\n",
    "# Directorio de los datos\n",
    "data_folder = '../Data/'\n",
    "#os.chdir('drive/MyDrive/Colab Notebooks/AST-ImageCaptioning/Notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuFdJ_qqiDj0"
   },
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.687461Z",
     "start_time": "2021-04-28T15:12:14.684804Z"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1619731176081,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "L2_XEe7jiDj1"
   },
   "outputs": [],
   "source": [
    "# Load an image and adjust it to InceptionV3 networks\n",
    "def image_to_v3_format(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55-8Qt6yiDj2"
   },
   "source": [
    "## Loading Images and Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:56:59.785560Z",
     "start_time": "2021-04-28T16:56:58.829222Z"
    },
    "executionInfo": {
     "elapsed": 3390,
     "status": "ok",
     "timestamp": 1619731943153,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "O1sASNiPiDj2"
   },
   "outputs": [],
   "source": [
    "# Process all the captions\n",
    "annotation_folder = '../Data/Captions/'\n",
    "PATH = '../Data/Images/Train/'\n",
    "with open(annotation_folder + 'captions_train2014.json', 'r') as f:\n",
    "    captions = json.load(f)\n",
    "    \n",
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in captions['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:57:03.291965Z",
     "start_time": "2021-04-28T16:57:03.230269Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3023,
     "status": "ok",
     "timestamp": 1619731943154,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "fIQmx2xOiDj2",
    "outputId": "e743efd5-0ee8-4250-9af8-dc1a9fd7b115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE NUMBER OF IMAGES IS 82783 AND THERE IS 414113 CAPTIONS\n"
     ]
    }
   ],
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "print('THE NUMBER OF IMAGES IS {} AND THERE IS {} CAPTIONS'.format(len(image_paths), len(captions['annotations'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:57:05.503291Z",
     "start_time": "2021-04-28T16:57:05.412503Z"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1619731945587,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "gb8-fvqKiDj3"
   },
   "outputs": [],
   "source": [
    "captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:57:20.361205Z",
     "start_time": "2021-04-28T16:57:09.259177Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9845,
     "status": "ok",
     "timestamp": 1619731999298,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "nEgxXTSPiDj3",
    "outputId": "17392e6b-7fee-4980-85b3-b3f0e99ae96c"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:59:08.498509Z",
     "start_time": "2021-04-28T16:59:08.224992Z"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1619732013478,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "HmqPZD0RiDj4"
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# It can be done for big batches because our vm has 128gb of ram\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(2**6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T18:41:41.208779Z",
     "start_time": "2021-04-28T17:01:08.193871Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90223,
     "status": "ok",
     "timestamp": 1619732107708,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "9VJT2JO9iDj4",
    "outputId": "c967ad6e-fe0e-4232-fd3d-63d9d9278c41"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.abspath('.') + '/' + image_folder + 'InceptionV3/'):\n",
    "    os.makedirs(os.path.abspath('.') + '/' + image_folder + 'InceptionV3/')\n",
    "    for img, path in tqdm(image_dataset):\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            path_of_feature = path_of_feature.split('/')[-1]\n",
    "            np.save(image_folder + 'InceptionV3/' + path_of_feature, bf.numpy())\n",
    "        del batch_features, bf, p, img, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1619732113011,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "AL7GcsaOiDj4"
   },
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(captions)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(captions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3pU7W7HiDj4"
   },
   "source": [
    "## Split: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1619732119388,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "B27QcpzpiDj4"
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1619732122332,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "nTZariEQiDj5",
    "outputId": "bcc06918-16d4-4965-a77d-efb798d8a18f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TRAIN IMAGES: 331291 AND NUMBER OF TRAIN CAPTIONS: 331291\n",
      "NUMBER OF TEST IMAGES: 82822 AND NUMBER OF TEST CAPTIONS: 82822\n"
     ]
    }
   ],
   "source": [
    "print('NUMBER OF TRAIN IMAGES: {} AND NUMBER OF TRAIN CAPTIONS: {}'.format(len(img_name_train), len(cap_train)))\n",
    "print('NUMBER OF TEST IMAGES: {} AND NUMBER OF TEST CAPTIONS: {}'.format(len(img_name_val), len(cap_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1619732124239,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "LIIu34E2iDj5"
   },
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1619732125828,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "rBYK0STniDj5"
   },
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_tensor, cap):\n",
    "    img_new_name = image_folder + 'InceptionV3/' + img_name.decode('utf-8').split('/')[-1]\n",
    "    img_new_name = str.encode(img_new_name)\n",
    "    img_tensor = np.load(img_new_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1287,
     "status": "ok",
     "timestamp": 1619732134522,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "jXmPu7dHiDj6"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1619732136176,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "Xn5OX-QziDj6"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1619732146912,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "dZgitAJBiDj6"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1619732148018,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "bfGwL57oiDj6"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1619732149018,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "bZ87pVBjiDj6"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1619732150304,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "0nOXn9LaiDj7"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1619732152013,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "ESJjGNs-iDj7"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./Checkpoints/Train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1619732153321,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "87EYgyAEiDj7"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1619732164795,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "_d1gUN_fiDj7"
   },
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440569,
     "status": "ok",
     "timestamp": 1619733609123,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "tPQ-3VroiDj7",
    "outputId": "2934deeb-df87-4b64-ded7-49f26051446e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:53<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 0 Loss 1.8396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-67f64c01b780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zip_files = os.listdir('../Data/Images/Zips/')\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for zippy in zip_files:\n",
    "        with zipfile.ZipFile('../Data/Images/Zips/' + zippy, 'r') as zip_ref:\n",
    "            new_temp = '../Data/Images/Temp/'\n",
    "            os.mkdir(new_temp)\n",
    "            zip_ref.extractall(new_temp)\n",
    "        \n",
    "        img_train_batch = os.listdir(new_temp + 'Data/Images/Train/')\n",
    "        image_dataset = tf.data.Dataset.from_tensor_slices([new_temp + 'Data/Images/Train/' + i for i in img_train_batch])\n",
    "        image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(128)\n",
    "        first = True\n",
    "        for img, path in tqdm(image_dataset):\n",
    "            batch_features = image_features_extract_model(img)\n",
    "            batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "            if first:\n",
    "                batch_features_total = tf.identity(batch_features)\n",
    "                first = False\n",
    "            else:\n",
    "                batch_features_total = tf.concat(axis=0, values=[batch_features_total,  batch_features])\n",
    "            \n",
    "        min_caption = np.inf\n",
    "        cap_train = []\n",
    "        for imgt in img_train_batch:\n",
    "            capt_len = len(img_to_cap_vector['../Data/Images/Train/' + imgt])\n",
    "            if capt_len < min_caption: min_caption = capt_len\n",
    "        for idx in range(min_caption):\n",
    "            if idx == 0:\n",
    "                batch_train = tf.identity(batch_features_total)\n",
    "            else:   \n",
    "                batch_train = tf.concat(axis=0, values = [batch_train, batch_features_total])\n",
    "                \n",
    "            for imgt in img_train_batch:\n",
    "                cap_train.append(img_to_cap_vector['../Data/Images/Train/' + imgt][idx])\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((batch_train, cap_train))\n",
    "\n",
    "        # Shuffle and batch\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        shutil.rmtree(new_temp)\n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "                print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(total_loss/\n",
    "                         num_steps)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchs de Zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number 5 Processed \n",
      "Batch Number 10 Processed \n",
      "Batch Number 15 Processed \n",
      "Batch Number 20 Processed \n",
      "Batch Number 25 Processed \n",
      "Batch Number 30 Processed \n",
      "Batch Number 35 Processed \n",
      "Batch Number 40 Processed \n",
      "Batch Number 45 Processed \n",
      "Batch Number 50 Processed \n"
     ]
    }
   ],
   "source": [
    "batches = 50\n",
    "total_train_keys = len(img_name_train_keys)\n",
    "try:\n",
    "    os.mkdir('../Data/Images/Zips/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for idx in range(batches):\n",
    "    lim_inf = (idx)*(total_train_keys // batches)\n",
    "    lim_sup = (idx+1)*(total_train_keys // batches)\n",
    "    if idx == (batches-1):\n",
    "        lim_s4up = total_train_keys\n",
    "    files_to_zip = img_name_train_keys[lim_inf:lim_sup]\n",
    "    with zipfile.ZipFile('../Data/Images/Zips/ZipBatch_{}.zip'.format(idx), 'w') as zipMe:        \n",
    "        for file in files_to_zip:\n",
    "            zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "    if (idx+1) % 5 == 0:\n",
    "        print('Batch Number {} Processed'.format(idx+1), '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZipBatch_33.zip\n",
      "ZipBatch_43.zip\n",
      "ZipBatch_7.zip\n",
      "ZipBatch_44.zip\n",
      "ZipBatch_10.zip\n",
      "ZipBatch_35.zip\n",
      "ZipBatch_2.zip\n",
      "ZipBatch_46.zip\n",
      "ZipBatch_38.zip\n",
      "ZipBatch_30.zip\n",
      "ZipBatch_40.zip\n",
      "ZipBatch_19.zip\n",
      "ZipBatch_16.zip\n",
      "ZipBatch_3.zip\n",
      "ZipBatch_34.zip\n",
      "ZipBatch_42.zip\n",
      "ZipBatch_1.zip\n",
      "ZipBatch_31.zip\n",
      "ZipBatch_29.zip\n",
      "ZipBatch_36.zip\n",
      "ZipBatch_14.zip\n",
      "ZipBatch_8.zip\n",
      "ZipBatch_20.zip\n",
      "ZipBatch_22.zip\n",
      "ZipBatch_17.zip\n",
      "ZipBatch_28.zip\n",
      "ZipBatch_13.zip\n",
      "ZipBatch_37.zip\n",
      "ZipBatch_0.zip\n",
      "ZipBatch_27.zip\n",
      "ZipBatch_47.zip\n",
      "ZipBatch_26.zip\n",
      "ZipBatch_11.zip\n",
      "ZipBatch_21.zip\n",
      "ZipBatch_39.zip\n",
      "ZipBatch_49.zip\n",
      "ZipBatch_12.zip\n",
      "ZipBatch_45.zip\n",
      "ZipBatch_41.zip\n",
      "ZipBatch_23.zip\n",
      "ZipBatch_6.zip\n",
      "ZipBatch_24.zip\n",
      "ZipBatch_4.zip\n",
      "ZipBatch_25.zip\n",
      "ZipBatch_18.zip\n",
      "ZipBatch_32.zip\n",
      "ZipBatch_9.zip\n",
      "ZipBatch_15.zip\n",
      "ZipBatch_48.zip\n",
      "ZipBatch_5.zip\n"
     ]
    }
   ],
   "source": [
    "files_to_zip = os.listdir('../Data/Images/Zips/')\n",
    "with zipfile.ZipFile('../Data/Images/SuperTrainZip.zip', 'w') as zipMe:        \n",
    "    for file in files_to_zip:\n",
    "        zipMe.write('../Data/Images/Zips/' + file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "u6ghkdDbiDj8"
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true,
    "id": "Pcp9_n1fiDj8"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('../Data/Images/Zips/ZipBatch_0.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../Data/Images/ZippyEx/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1324"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('../Data/Images/ZippyEx/Data/Images/Train/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '../Data/Images/ZippyEx/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImageCaptioningDemo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
