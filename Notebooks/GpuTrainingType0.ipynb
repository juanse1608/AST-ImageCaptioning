{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"GpuTrainingType0.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"environment":{"name":"tf2-gpu.2-4.m65","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"},"kernelspec":{"display_name":"Python [conda env:root] *","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"206.2px"},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"JfJZLpl-iDjs","toc":true},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Loading-Images-and-Captions\" data-toc-modified-id=\"Loading-Images-and-Captions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loading Images and Captions</a></span></li><li><span><a href=\"#Split:-Train-and-Test\" data-toc-modified-id=\"Split:-Train-and-Test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Split: Train and Test</a></span></li></ul></div>"]},{"cell_type":"markdown","metadata":{"id":"Ob7dvTLiLjr8"},"source":["# Image Captioning - Advanced Statistics Topics\n","\n","In this notebook we adecuate the enviroment to sue the GPU provided by Google Colab. Unfortunately, the storage (CPU) capacity of the vm is too low and we have to read, process, train the model and delete little batches of the training images.   "]},{"cell_type":"markdown","metadata":{"id":"R6lCo231DGwz"},"source":["## Import Libraries\n","\n","In this section we import the requiere libraries and establish the connection with Google Drive."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T15:12:14.675664Z","start_time":"2021-04-28T15:12:11.016400Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"WdtmqKdzMA0W","executionInfo":{"status":"ok","timestamp":1619907527812,"user_tz":300,"elapsed":409,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"9f345eca-8bba-43a6-bf7b-e000fd614d14"},"source":["# Se importan las librerias necesarias\n","import numpy as np\n","import pandas as  pd\n","import os\n","import gzip\n","import timeit\n","import shutil\n","import json\n","import collections\n","import zipfile\n","import random\n","import time\n","from PIL import Image\n","from tqdm import tqdm\n","from google.colab import drive # Necesario para GoogleColab\n","\n","# Tensorflow 2.x\n","import tensorflow as tf # Not required for this notebook \n","print('TENSORFLOW VERSION: {}'.format(tf.__version__))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["TENSORFLOW VERSION: 2.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T15:12:14.679133Z","start_time":"2021-04-28T15:12:14.677248Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"zp8cp8wMMBhN","executionInfo":{"status":"ok","timestamp":1619907031720,"user_tz":300,"elapsed":116602,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"a8b776dc-98a6-4db4-cbca-d48322975d48"},"source":["# Se fija el directorio referencia con el se conecta colab a drive # Necesario para GoogleColab\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T15:12:14.683150Z","start_time":"2021-04-28T15:12:14.681323Z"},"id":"b9mNBsu2ENCt","executionInfo":{"status":"ok","timestamp":1619907034125,"user_tz":300,"elapsed":695,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# Se guarda en una variable el directorio del curso\n","\n","# Directorio de los datos\n","data_folder = '../Data/'\n","os.chdir('drive/MyDrive/Colab Notebooks/AST-ImageCaptioning/Notebooks/')"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuFdJ_qqiDj0"},"source":["## Functions and Classes"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T15:12:14.687461Z","start_time":"2021-04-28T15:12:14.684804Z"},"id":"L2_XEe7jiDj1","executionInfo":{"status":"ok","timestamp":1619907038916,"user_tz":300,"elapsed":422,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# Load an image and adjust it to InceptionV3 networks\n","def image_to_v3_format(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (299, 299))\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","    return img, image_path"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55-8Qt6yiDj2"},"source":["## Loading Images and Captions"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T16:56:59.785560Z","start_time":"2021-04-28T16:56:58.829222Z"},"id":"O1sASNiPiDj2","executionInfo":{"status":"ok","timestamp":1619907057528,"user_tz":300,"elapsed":3443,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# Process all the captions\n","annotation_folder = '../Data/Captions/'\n","PATH = '../Data/Images/Train/'\n","with open(annotation_folder + 'captions_train2014.json', 'r') as f:\n","    captions = json.load(f)\n","    \n","# Group all captions together having the same image ID.\n","image_path_to_caption = collections.defaultdict(list)\n","for val in captions['annotations']:\n","    caption = f\"<start> {val['caption']} <end>\"\n","    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n","    image_path_to_caption[image_path].append(caption)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T16:57:03.291965Z","start_time":"2021-04-28T16:57:03.230269Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"fIQmx2xOiDj2","executionInfo":{"status":"ok","timestamp":1619907155239,"user_tz":300,"elapsed":414,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"3e07c078-7ea9-4e0d-a27c-58f0da45b273"},"source":["image_paths = list(image_path_to_caption.keys())\n","random.shuffle(image_paths)\n","print('THE NUMBER OF IMAGES IS {} AND THERE IS {} CAPTIONS'.format(len(image_paths), len(captions['annotations'])))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["THE NUMBER OF IMAGES IS 82783 AND THERE IS 414113 CAPTIONS\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T16:57:05.503291Z","start_time":"2021-04-28T16:57:05.412503Z"},"id":"gb8-fvqKiDj3","executionInfo":{"status":"ok","timestamp":1619907173810,"user_tz":300,"elapsed":495,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["captions = []\n","img_name_vector = []\n","\n","for image_path in image_paths:\n","    caption_list = image_path_to_caption[image_path]\n","    captions.extend(caption_list)\n","    img_name_vector.extend([image_path] * len(caption_list))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T16:57:20.361205Z","start_time":"2021-04-28T16:57:09.259177Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"nEgxXTSPiDj3","executionInfo":{"status":"ok","timestamp":1619907187419,"user_tz":300,"elapsed":8252,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"edb9eab5-b2f8-4392-f0ae-2425bb2ba61c"},"source":["image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","87916544/87910968 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-28T16:59:08.498509Z","start_time":"2021-04-28T16:59:08.224992Z"},"id":"HmqPZD0RiDj4","executionInfo":{"status":"ok","timestamp":1619907192217,"user_tz":300,"elapsed":788,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# Get unique images\n","encode_train = sorted(set(img_name_vector))\n","\n","# It can be done for big batches because our vm has 128gb of ram\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(2**6) "],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"AL7GcsaOiDj4","executionInfo":{"status":"ok","timestamp":1619907217724,"user_tz":300,"elapsed":12986,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# Find the maximum length of any caption in our dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","# Choose the top 5000 words from the vocabulary\n","top_k = 5000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n","tokenizer.fit_on_texts(captions)\n","\n","tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'\n","\n","# Create the tokenized vectors\n","train_seqs = tokenizer.texts_to_sequences(captions)\n","\n","# Pad each vector to the max_length of the captions\n","# If you do not provide a max_length value, pad_sequences calculates it automatically\n","cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n","\n","# Calculates the max_length, which is used to store the attention weights\n","max_length = calc_max_length(train_seqs)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3pU7W7HiDj4"},"source":["## Split: Train and Test"]},{"cell_type":"code","metadata":{"id":"B27QcpzpiDj4","executionInfo":{"status":"ok","timestamp":1619907235256,"user_tz":300,"elapsed":1025,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["img_to_cap_vector = collections.defaultdict(list)\n","for img, cap in zip(img_name_vector, cap_vector):\n","    img_to_cap_vector[img].append(cap)\n","\n","# Create training and validation sets using an 80-20 split randomly.\n","img_keys = list(img_to_cap_vector.keys())\n","random.shuffle(img_keys)\n","\n","slice_index = int(len(img_keys)*0.8)\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","\n","img_name_train = []\n","cap_train = []\n","for imgt in img_name_train_keys:\n","    capt_len = len(img_to_cap_vector[imgt])\n","    img_name_train.extend([imgt] * capt_len)\n","    cap_train.extend(img_to_cap_vector[imgt])\n","\n","img_name_val = []\n","cap_val = []\n","for imgv in img_name_val_keys:\n","    capv_len = len(img_to_cap_vector[imgv])\n","    img_name_val.extend([imgv] * capv_len)\n","    cap_val.extend(img_to_cap_vector[imgv])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTZariEQiDj5","executionInfo":{"status":"ok","timestamp":1619907235256,"user_tz":300,"elapsed":639,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"cb3eab61-dae1-41ab-c057-1064ac483f40"},"source":["print('NUMBER OF TRAIN IMAGES: {} AND NUMBER OF TRAIN CAPTIONS: {}'.format(len(img_name_train), len(cap_train)))\n","print('NUMBER OF TEST IMAGES: {} AND NUMBER OF TEST CAPTIONS: {}'.format(len(img_name_val), len(cap_val)))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["NUMBER OF TRAIN IMAGES: 331290 AND NUMBER OF TRAIN CAPTIONS: 331290\n","NUMBER OF TEST IMAGES: 82823 AND NUMBER OF TEST CAPTIONS: 82823\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LIIu34E2iDj5","executionInfo":{"status":"ok","timestamp":1619907237580,"user_tz":300,"elapsed":266,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# Feel free to change these parameters according to your system's configuration\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","vocab_size = top_k + 1\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","features_shape = 2048\n","attention_features_shape = 64"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xn5OX-QziDj6","executionInfo":{"status":"ok","timestamp":1619907300991,"user_tz":300,"elapsed":510,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["class BahdanauAttention(tf.keras.Model):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, features, hidden):\n","        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","        # hidden shape == (batch_size, hidden_size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","        # attention_hidden_layer shape == (batch_size, 64, units)\n","        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","                                             self.W2(hidden_with_time_axis)))\n","\n","        # score shape == (batch_size, 64, 1)\n","        # This gives you an unnormalized score for each image feature.\n","        score = self.V(attention_hidden_layer)\n","\n","        # attention_weights shape == (batch_size, 64, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * features\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"dZgitAJBiDj6","executionInfo":{"status":"ok","timestamp":1619907302970,"user_tz":300,"elapsed":421,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["class CNN_Encoder(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        x = self.fc(x)\n","        x = tf.nn.relu(x)\n","        return x"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfGwL57oiDj6","executionInfo":{"status":"ok","timestamp":1619907304637,"user_tz":300,"elapsed":446,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["class RNN_Decoder(tf.keras.Model):\n","    def __init__(self, embedding_dim, units, vocab_size):\n","        super(RNN_Decoder, self).__init__()\n","        self.units = units\n","\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        self.fc1 = tf.keras.layers.Dense(self.units)\n","        self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","        self.attention = BahdanauAttention(self.units)\n","\n","    def call(self, x, features, hidden):\n","        # defining attention as a separate model\n","        context_vector, attention_weights = self.attention(features, hidden)\n","\n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","\n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","\n","        # shape == (batch_size, max_length, hidden_size)\n","        x = self.fc1(output)\n","\n","        # x shape == (batch_size * max_length, hidden_size)\n","        x = tf.reshape(x, (-1, x.shape[2]))\n","\n","        # output shape == (batch_size * max_length, vocab)\n","        x = self.fc2(x)\n","\n","        return x, state, attention_weights\n","\n","    def reset_state(self, batch_size):\n","        return tf.zeros((batch_size, self.units))"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZ87pVBjiDj6","executionInfo":{"status":"ok","timestamp":1619907307438,"user_tz":300,"elapsed":499,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"0nOXn9LaiDj7","executionInfo":{"status":"ok","timestamp":1619907309948,"user_tz":300,"elapsed":300,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESJjGNs-iDj7","executionInfo":{"status":"ok","timestamp":1619907428119,"user_tz":300,"elapsed":408,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["checkpoint_path = \"../Data/TrainCheckpoints/TrainType0\"\n","ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"87EYgyAEiDj7","executionInfo":{"status":"ok","timestamp":1619907432702,"user_tz":300,"elapsed":463,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    # restoring the latest checkpoint in checkpoint_path\n","    ckpt.restore(ckpt_manager.latest_checkpoint)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"_d1gUN_fiDj7","executionInfo":{"status":"ok","timestamp":1619907437084,"user_tz":300,"elapsed":434,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}}},"source":["# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []\n","\n","@tf.function\n","def train_step(img_tensor, target):\n","    loss = 0\n","\n","    # initializing the hidden state for each batch\n","    # because the captions are not related from image to image\n","    hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","    with tf.GradientTape() as tape:\n","        features = encoder(img_tensor)\n","\n","        for i in range(1, target.shape[1]):\n","            # passing the features through the decoder\n","            predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","            loss += loss_function(target[:, i], predictions)\n","\n","            # using teacher forcing\n","            dec_input = tf.expand_dims(target[:, i], 1)\n","\n","    total_loss = (loss / int(target.shape[1]))\n","\n","    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","    gradients = tape.gradient(loss, trainable_variables)\n","\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","    return loss, total_loss"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tPQ-3VroiDj7","executionInfo":{"status":"error","timestamp":1619913598310,"user_tz":300,"elapsed":3878750,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"3af00b66-bb2d-4a05-f3e5-776af15ec7a0"},"source":["zip_files = os.listdir('../Data/Images/Zips/')\n","EPOCHS = 20\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","    \n","    real_batch = 0\n","    for zippy in zip_files:\n","        with zipfile.ZipFile('../Data/Images/Zips/' + zippy, 'r') as zip_ref:\n","            new_temp = '../Data/Images/Temp/'\n","            os.mkdir(new_temp)\n","            zip_ref.extractall(new_temp)\n","\n","        img_train_batch = os.listdir(new_temp + 'Data/Images/Train/')\n","        image_dataset = tf.data.Dataset.from_tensor_slices([new_temp + 'Data/Images/Train/' + i for i in img_train_batch])\n","        image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","        first = True\n","        for img, path in image_dataset:\n","            batch_features = image_features_extract_model(img)\n","            batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n","            if first:\n","                batch_features_total = tf.identity(batch_features)\n","                first = False\n","            else:\n","                batch_features_total = tf.concat(axis=0, values=[batch_features_total,  batch_features])\n","            \n","        min_caption = np.inf\n","        cap_train = []\n","        for imgt in img_train_batch:\n","            capt_len = len(img_to_cap_vector['../Data/Images/Train/' + imgt])\n","            if capt_len < min_caption: min_caption = capt_len\n","        min_caption = 2 # Colab get OOM with the min_caption >= 3\n","        for idx in range(min_caption):\n","            if idx == 0:\n","                batch_train = tf.identity(batch_features_total)\n","            else:   \n","\n","                batch_train = tf.concat(axis=0, values=[batch_train, batch_features_total])\n","                \n","            for imgt in img_train_batch:\n","                cap_train.append(img_to_cap_vector['../Data/Images/Train/' + imgt][idx])\n","\n","        dataset = tf.data.Dataset.from_tensor_slices((batch_train, cap_train))\n","\n","        # Shuffle and batch\n","        dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","        \n","      \n","        shutil.rmtree(new_temp)\n","        \n","        for (batch, (img_tensor, target)) in enumerate(dataset):\n","            batch_loss, t_loss = train_step(img_tensor, target)\n","            total_loss += t_loss\n","            real_batch += 1\n","            if real_batch % 100 == 0:\n","                average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n","                print(f'Epoch {epoch+1} Batch {real_batch} Loss {average_batch_loss:.4f}')\n","        # storing the epoch end loss value to plot later\n","        loss_plot.append(total_loss/num_steps)\n","\n","    if epoch % 1 == 0:\n","        ckpt_manager.save()\n","\n","    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n","    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 100 Loss 0.6092\n","Epoch 1 Batch 200 Loss 0.6891\n","Epoch 1 Batch 300 Loss 0.6646\n","Epoch 1 Batch 400 Loss 0.6236\n","Epoch 1 Batch 500 Loss 0.7345\n","Epoch 1 Batch 600 Loss 0.6345\n","Epoch 1 Batch 700 Loss 0.6344\n","Epoch 1 Batch 800 Loss 0.6047\n","Epoch 1 Batch 900 Loss 0.7420\n","Epoch 1 Batch 1000 Loss 0.7920\n","Epoch 1 Batch 1100 Loss 0.6598\n","Epoch 1 Batch 1200 Loss 0.6581\n","Epoch 1 Batch 1300 Loss 0.5825\n","Epoch 1 Batch 1400 Loss 0.5495\n","Epoch 1 Batch 1500 Loss 0.6275\n","Epoch 1 Batch 1600 Loss 0.6545\n","Epoch 1 Batch 1700 Loss 0.5883\n","Epoch 1 Batch 1800 Loss 0.6513\n","Epoch 1 Batch 1900 Loss 0.6246\n","Epoch 1 Batch 2000 Loss 0.5561\n","Epoch 1 Batch 2100 Loss 0.5084\n","Epoch 1 Batch 2200 Loss 0.7304\n","Epoch 1 Batch 2300 Loss 0.7075\n","Epoch 1 Batch 2400 Loss 0.6274\n","Epoch 1 Batch 2500 Loss 0.5540\n","Epoch 1 Batch 2600 Loss 0.6554\n","Epoch 1 Batch 2700 Loss 0.6225\n","Epoch 1 Batch 2800 Loss 0.6313\n","Epoch 1 Batch 2900 Loss 0.7246\n","Epoch 1 Batch 3000 Loss 0.6092\n","Epoch 1 Batch 3100 Loss 0.5905\n","Epoch 1 Batch 3200 Loss 0.6840\n","Epoch 1 Batch 3300 Loss 0.6244\n","Epoch 1 Batch 3400 Loss 0.6792\n","Epoch 1 Batch 3500 Loss 0.7197\n","Epoch 1 Batch 3600 Loss 0.6603\n","Epoch 1 Batch 3700 Loss 0.6000\n","Epoch 1 Batch 3800 Loss 0.6526\n","Epoch 1 Batch 3900 Loss 0.6036\n","Epoch 1 Batch 4000 Loss 0.6827\n","Epoch 1 Batch 4100 Loss 0.6256\n","Epoch 1 Loss 0.256289\n","Time taken for 1 epoch 3460.34 sec\n","\n","Epoch 2 Batch 100 Loss 0.5932\n","Epoch 2 Batch 200 Loss 0.5670\n","Epoch 2 Batch 300 Loss 0.6398\n","Epoch 2 Batch 400 Loss 0.5785\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-e0d76c242756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnew_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Data/Images/Temp/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimg_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_temp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Data/Images/Train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTLeg1tcX_qN","executionInfo":{"status":"ok","timestamp":1619913785314,"user_tz":300,"elapsed":1751,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"2ce2dbe1-d9fe-4038-b376-59b641b71b20"},"source":["!ls ../Data"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Captions  Images  TrainCheckpoints\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"u6ghkdDbiDj8"},"source":["# End"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oxLDKM59p-t","executionInfo":{"status":"ok","timestamp":1619909719553,"user_tz":300,"elapsed":9149,"user":{"displayName":"Juan Sebastian Corredor Rodriguez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64","userId":"12217655037166393722"}},"outputId":"4edf3222-c1d0-48de-b531-1a2dd75b63fb"},"source":["import shutil\n","\n","dir_path = '../Data/Images/Temp/'\n","\n","try:\n","    shutil.rmtree(dir_path)\n","    print('ELIMINADO')\n","except OSError as e:\n","    print(\"Error: %s : %s\" % (dir_path, e.strerror))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["ELIMINADO\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v4MUXzjf9p-t"},"source":[""],"execution_count":null,"outputs":[]}]}