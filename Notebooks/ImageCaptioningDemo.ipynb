{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfJZLpl-iDjs",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Loading-Images-and-Captions\" data-toc-modified-id=\"Loading-Images-and-Captions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loading Images and Captions</a></span></li><li><span><a href=\"#Split:-Train-and-Test\" data-toc-modified-id=\"Split:-Train-and-Test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Split: Train and Test</a></span></li><li><span><a href=\"#Batchs-de-Zips\" data-toc-modified-id=\"Batchs-de-Zips-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Batchs de Zips</a></span></li><li><span><a href=\"#Get-bucket-metadata\" data-toc-modified-id=\"Get-bucket-metadata-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Get bucket metadata</a></span></li><li><span><a href=\"#List-blobs-in-a-bucket\" data-toc-modified-id=\"List-blobs-in-a-bucket-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>List blobs in a bucket</a></span></li><li><span><a href=\"#Save-Model-Parameters\" data-toc-modified-id=\"Save-Model-Parameters-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Save Model Parameters</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob7dvTLiLjr8"
   },
   "source": [
    "# Image Captioning - Advanced Statistics Topics\n",
    "\n",
    "In this notebook, we train several models and save the results of its metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6lCo231DGwz"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "In this section we import the requiere libraries and establish the connection with Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T00:01:30.931855Z",
     "start_time": "2021-05-03T00:01:30.927805Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1619731137887,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "WdtmqKdzMA0W",
    "outputId": "8cdd771f-6887-4be6-979a-ca5fbb7d7855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORFLOW VERSION: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Se importan las librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import gzip\n",
    "import timeit\n",
    "import json\n",
    "import collections\n",
    "import zipfile\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "# from google.colab import drive # Necesario para GoogleColab\n",
    "\n",
    "# Tensorflow 2.x\n",
    "import tensorflow as tf # Not required for this notebook \n",
    "print('TENSORFLOW VERSION: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:17.646767Z",
     "start_time": "2021-05-02T23:22:17.644708Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35290,
     "status": "ok",
     "timestamp": 1619731172086,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "zp8cp8wMMBhN",
    "outputId": "57a31698-4088-4552-ba5a-876f7e37bf37"
   },
   "outputs": [],
   "source": [
    "# Se fija el directorio referencia con el se conecta colab a drive # Necesario para GoogleColab\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:18.005236Z",
     "start_time": "2021-05-02T23:22:18.003124Z"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1619731174450,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "b9mNBsu2ENCt"
   },
   "outputs": [],
   "source": [
    "# Se guarda en una variable el directorio del curso\n",
    "\n",
    "# Directorio de los datos\n",
    "data_folder = '../Data/'\n",
    "#os.chdir('drive/MyDrive/Colab Notebooks/AST-ImageCaptioning/Notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuFdJ_qqiDj0"
   },
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:18.772233Z",
     "start_time": "2021-05-02T23:22:18.769217Z"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1619731176081,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "L2_XEe7jiDj1"
   },
   "outputs": [],
   "source": [
    "# Load an image and adjust it to InceptionV3 networks\n",
    "def image_to_v3_format(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5xxl12mE6Z8"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section we process the images and its respective captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:19.876279Z",
     "start_time": "2021-05-02T23:22:19.872442Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9443,
     "status": "ok",
     "timestamp": 1619731188590,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "lOiRu0uiES94",
    "outputId": "7c1afc3c-0d00-4fd5-fa7e-7af5b4a289af"
   },
   "outputs": [],
   "source": [
    "# Download caption annotation files\n",
    "annotation_folder = data_folder + '/Captions/'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + annotation_folder):\n",
    "    annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                              cache_subdir=os.path.abspath('.') + '/' + data_folder,\n",
    "                              origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                              extract=True)\n",
    "    os.rename(data_folder + 'annotations/', annotation_folder)\n",
    "    annotation_file = os.path.dirname(annotation_zip) + '/annotations/captions_train2014.json'\n",
    "    os.remove(annotation_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:20.617640Z",
     "start_time": "2021-05-02T23:22:20.614040Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 721200,
     "status": "ok",
     "timestamp": 1619731907336,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "x6DjBesHiDj2",
    "outputId": "b4a53e23-8e94-4786-f39e-bdad637b63d5"
   },
   "outputs": [],
   "source": [
    "# Download image files\n",
    "image_folder = data_folder + 'Images/'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + image_folder):\n",
    "    image_zip = tf.keras.utils.get_file('Train.zip',\n",
    "                                      cache_subdir=os.path.abspath('.') + '/' + image_folder,\n",
    "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract=True)\n",
    "#     os.rename(image_folder + 'train2014/', image_folder + 'Train/') # Better to change the name directly\n",
    "    PATH = image_folder + 'Train/'\n",
    "    os.remove(image_zip)\n",
    "else:\n",
    "    PATH = image_folder + 'Train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55-8Qt6yiDj2"
   },
   "source": [
    "## Loading Images and Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:23.068348Z",
     "start_time": "2021-05-02T23:22:22.119430Z"
    },
    "executionInfo": {
     "elapsed": 3390,
     "status": "ok",
     "timestamp": 1619731943153,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "O1sASNiPiDj2"
   },
   "outputs": [],
   "source": [
    "# Process all the captions\n",
    "with open(annotation_folder + 'captions_train2014.json', 'r') as f:\n",
    "    captions = json.load(f)\n",
    "    \n",
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in captions['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:23.913998Z",
     "start_time": "2021-05-02T23:22:23.855303Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3023,
     "status": "ok",
     "timestamp": 1619731943154,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "fIQmx2xOiDj2",
    "outputId": "e743efd5-0ee8-4250-9af8-dc1a9fd7b115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE NUMBER OF IMAGES IS 82783 AND THERE IS 414113 CAPTIONS\n"
     ]
    }
   ],
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# Select the first 6000 image_paths from the shuffled set.\n",
    "# Approximately each image id has 5 captions associated with it, so that will\n",
    "# lead to 30,000 examples.\n",
    "train_image_paths = image_paths.copy()#[:6000]\n",
    "print('THE NUMBER OF IMAGES IS {} AND THERE IS {} CAPTIONS'.format(len(train_image_paths), len(captions['annotations'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:25.483179Z",
     "start_time": "2021-05-02T23:22:25.384445Z"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1619731945587,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "gb8-fvqKiDj3"
   },
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    train_captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:26.881245Z",
     "start_time": "2021-05-02T23:22:26.462872Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1619731986359,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "3h8z9XQRiDj3",
    "outputId": "bd29cb63-d7bb-43f6-fe07-ad008a04515b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: <start> man catching a Frisbee while his leg folds backwards <end>\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/Images/Train/COCO_train2014_000000274209.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-07744ee3c0b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1608\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Caption:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_captions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/ML/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2878\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2879\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/Images/Train/COCO_train2014_000000274209.jpg'"
     ]
    }
   ],
   "source": [
    "sample_image = 1608\n",
    "print('Caption:', train_captions[sample_image])\n",
    "Image.open(img_name_vector[sample_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:22:34.347730Z",
     "start_time": "2021-05-02T23:22:32.820981Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9845,
     "status": "ok",
     "timestamp": 1619731999298,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "nEgxXTSPiDj3",
    "outputId": "17392e6b-7fee-4980-85b3-b3f0e99ae96c"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:59:08.498509Z",
     "start_time": "2021-04-28T16:59:08.224992Z"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1619732013478,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "HmqPZD0RiDj4"
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# It can be done for big batches because our vm has 128gb of ram\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(2**6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T18:41:41.208779Z",
     "start_time": "2021-04-28T17:01:08.193871Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90223,
     "status": "ok",
     "timestamp": 1619732107708,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "9VJT2JO9iDj4",
    "outputId": "c967ad6e-fe0e-4232-fd3d-63d9d9278c41"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.abspath('.') + '/' + image_folder + 'InceptionV3/'):\n",
    "    os.makedirs(os.path.abspath('.') + '/' + image_folder + 'InceptionV3/')\n",
    "    for img, path in tqdm(image_dataset):\n",
    "        batch_features = image_features_extract_model(img)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            path_of_feature = path_of_feature.split('/')[-1]\n",
    "            np.save(image_folder + 'InceptionV3/' + path_of_feature, bf.numpy())\n",
    "        del batch_features, bf, p, img, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:23:07.362408Z",
     "start_time": "2021-05-02T23:22:57.434673Z"
    },
    "executionInfo": {
     "elapsed": 1538,
     "status": "ok",
     "timestamp": 1619732113011,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "AL7GcsaOiDj4"
   },
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3pU7W7HiDj4"
   },
   "source": [
    "## Split: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1619732119388,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "B27QcpzpiDj4"
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1619732122332,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "nTZariEQiDj5",
    "outputId": "bcc06918-16d4-4965-a77d-efb798d8a18f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TRAIN IMAGES: 331293 AND NUMBER OF TRAIN CAPTIONS: 331293\n",
      "NUMBER OF TEST IMAGES: 82820 AND NUMBER OF TEST CAPTIONS: 82820\n"
     ]
    }
   ],
   "source": [
    "print('NUMBER OF TRAIN IMAGES: {} AND NUMBER OF TRAIN CAPTIONS: {}'.format(len(img_name_train), len(cap_train)))\n",
    "print('NUMBER OF TEST IMAGES: {} AND NUMBER OF TEST CAPTIONS: {}'.format(len(img_name_val), len(cap_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T23:24:27.910913Z",
     "start_time": "2021-05-02T23:24:27.908279Z"
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1619732124239,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "LIIu34E2iDj5"
   },
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1619732125828,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "rBYK0STniDj5"
   },
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_new_name = image_folder + 'InceptionV3/' + img_name.decode('utf-8').split('/')[-1]\n",
    "    img_new_name = str.encode(img_new_name)\n",
    "    img_tensor = np.load(img_new_name.decode('utf-8') + '.npy')\n",
    "    return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1287,
     "status": "ok",
     "timestamp": 1619732134522,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "jXmPu7dHiDj6"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1619732136176,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "Xn5OX-QziDj6"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1619732146912,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "dZgitAJBiDj6"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1619732148018,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "bfGwL57oiDj6"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1619732149018,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "bZ87pVBjiDj6"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1619732150304,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "0nOXn9LaiDj7"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "ok",
     "timestamp": 1619732152013,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "ESJjGNs-iDj7"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"../Checkpoints/Train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1619732153321,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "87EYgyAEiDj7"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1619732164795,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "_d1gUN_fiDj7"
   },
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440569,
     "status": "ok",
     "timestamp": 1619733609123,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "tPQ-3VroiDj7",
    "outputId": "2934deeb-df87-4b64-ded7-49f26051446e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 0 Loss 1.6389\n",
      "Epoch 2 Batch 100 Loss 0.9878\n",
      "Epoch 2 Batch 200 Loss 1.0378\n",
      "Epoch 2 Batch 300 Loss 0.9278\n",
      "Epoch 2 Batch 400 Loss 0.8907\n",
      "Epoch 2 Batch 500 Loss 0.7636\n",
      "Epoch 2 Batch 600 Loss 0.7329\n",
      "Epoch 2 Batch 700 Loss 0.7543\n",
      "Epoch 2 Loss 0.886059\n",
      "Time taken for 1 epoch 2959.04 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.8180\n",
      "Epoch 3 Batch 100 Loss 0.7623\n",
      "Epoch 3 Batch 200 Loss 0.7075\n",
      "Epoch 3 Batch 300 Loss 0.6795\n",
      "Epoch 3 Batch 400 Loss 0.6513\n",
      "Epoch 3 Batch 500 Loss 0.6588\n",
      "Epoch 3 Batch 600 Loss 0.6173\n",
      "Epoch 3 Batch 700 Loss 0.6628\n",
      "Epoch 3 Loss 0.725411\n",
      "Time taken for 1 epoch 2382.37 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6654\n",
      "Epoch 4 Batch 100 Loss 0.7556\n",
      "Epoch 4 Batch 200 Loss 0.7017\n",
      "Epoch 4 Batch 300 Loss 0.6862\n",
      "Epoch 4 Batch 400 Loss 0.8255\n",
      "Epoch 4 Batch 500 Loss 0.6612\n",
      "Epoch 4 Batch 600 Loss 0.6536\n",
      "Epoch 4 Batch 700 Loss 0.7428\n",
      "Epoch 4 Loss 0.664200\n",
      "Time taken for 1 epoch 2318.35 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.7098\n",
      "Epoch 5 Batch 100 Loss 0.5878\n",
      "Epoch 5 Batch 200 Loss 0.6312\n",
      "Epoch 5 Batch 300 Loss 0.6577\n",
      "Epoch 5 Batch 400 Loss 0.6386\n",
      "Epoch 5 Batch 500 Loss 0.6013\n",
      "Epoch 5 Batch 600 Loss 0.5981\n",
      "Epoch 5 Batch 700 Loss 0.6144\n",
      "Epoch 5 Loss 0.617779\n",
      "Time taken for 1 epoch 2357.97 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4089fda33bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss/\n",
    "                     num_steps)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchs de Zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number 5 Processed \n",
      "Batch Number 10 Processed \n",
      "Batch Number 15 Processed \n",
      "Batch Number 20 Processed \n",
      "Batch Number 25 Processed \n",
      "Batch Number 30 Processed \n",
      "Batch Number 35 Processed \n",
      "Batch Number 40 Processed \n",
      "Batch Number 45 Processed \n",
      "Batch Number 50 Processed \n"
     ]
    }
   ],
   "source": [
    "batches = 50\n",
    "total_train_keys = len(img_name_train_keys)\n",
    "try:\n",
    "    os.mkdir('../Data/Images/Zips/')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for idx in range(batches):\n",
    "    lim_inf = (idx)*(total_train_keys // batches)\n",
    "    lim_sup = (idx+1)*(total_train_keys // batches)\n",
    "    if idx == (batches-1):\n",
    "        lim_s4up = total_train_keys\n",
    "    files_to_zip = img_name_train_keys[lim_inf:lim_sup]\n",
    "    with zipfile.ZipFile('../Data/Images/Zips/ZipBatch_{}.zip'.format(idx), 'w') as zipMe:        \n",
    "        for file in files_to_zip:\n",
    "            zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "    if (idx+1) % 5 == 0:\n",
    "        print('Batch Number {} Processed'.format(idx+1), '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZipBatch_33.zip\n",
      "ZipBatch_43.zip\n",
      "ZipBatch_7.zip\n",
      "ZipBatch_44.zip\n",
      "ZipBatch_10.zip\n",
      "ZipBatch_35.zip\n",
      "ZipBatch_2.zip\n",
      "ZipBatch_46.zip\n",
      "ZipBatch_38.zip\n",
      "ZipBatch_30.zip\n",
      "ZipBatch_40.zip\n",
      "ZipBatch_19.zip\n",
      "ZipBatch_16.zip\n",
      "ZipBatch_3.zip\n",
      "ZipBatch_34.zip\n",
      "ZipBatch_42.zip\n",
      "ZipBatch_1.zip\n",
      "ZipBatch_31.zip\n",
      "ZipBatch_29.zip\n",
      "ZipBatch_36.zip\n",
      "ZipBatch_14.zip\n",
      "ZipBatch_8.zip\n",
      "ZipBatch_20.zip\n",
      "ZipBatch_22.zip\n",
      "ZipBatch_17.zip\n",
      "ZipBatch_28.zip\n",
      "ZipBatch_13.zip\n",
      "ZipBatch_37.zip\n",
      "ZipBatch_0.zip\n",
      "ZipBatch_27.zip\n",
      "ZipBatch_47.zip\n",
      "ZipBatch_26.zip\n",
      "ZipBatch_11.zip\n",
      "ZipBatch_21.zip\n",
      "ZipBatch_39.zip\n",
      "ZipBatch_49.zip\n",
      "ZipBatch_12.zip\n",
      "ZipBatch_45.zip\n",
      "ZipBatch_41.zip\n",
      "ZipBatch_23.zip\n",
      "ZipBatch_6.zip\n",
      "ZipBatch_24.zip\n",
      "ZipBatch_4.zip\n",
      "ZipBatch_25.zip\n",
      "ZipBatch_18.zip\n",
      "ZipBatch_32.zip\n",
      "ZipBatch_9.zip\n",
      "ZipBatch_15.zip\n",
      "ZipBatch_48.zip\n",
      "ZipBatch_5.zip\n"
     ]
    }
   ],
   "source": [
    "files_to_zip = os.listdir('../Data/Images/Zips/')\n",
    "with zipfile.ZipFile('../Data/Images/SuperTrainZip.zip', 'w') as zipMe:        \n",
    "    for file in files_to_zip:\n",
    "        zipMe.write('../Data/Images/Zips/' + file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `storage.Client` object uses your default project. Alternatively, you can specify a project in the `Client` constructor. For more information about how the default project is determined, see the [google-auth documentation](https://google-auth.readthedocs.io/en/latest/reference/google.auth.html).\n",
    "\n",
    "Run the following to create a client with your default project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client created using default project: imagecaptioning-311723\n"
     ]
    }
   ],
   "source": [
    "client = storage.Client()\n",
    "print(\"Client created using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets in imagecaptioning-311723:\n",
      "\timage-captioning-data\n"
     ]
    }
   ],
   "source": [
    "buckets = client.list_buckets()\n",
    "\n",
    "print(\"Buckets in {}:\".format(client.project))\n",
    "for item in buckets:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bucket metadata\n",
    "\n",
    "The next cell shows how to get information on metadata of your Cloud Storage buckets.\n",
    "\n",
    "To learn more about specific bucket properties, see [Bucket locations](https://cloud.google.com/storage/docs/locations) and [Storage classes](https://cloud.google.com/storage/docs/storage-classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name: image-captioning-data\n",
      "Bucket location: US-EAST1\n",
      "Bucket storage class: STANDARD\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'image-captioning-data'\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "print(\"Bucket name: {}\".format(bucket.name))\n",
    "print(\"Bucket location: {}\".format(bucket.location))\n",
    "print(\"Bucket storage class: {}\".format(bucket.storage_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to image-captioning-data.\n"
     ]
    }
   ],
   "source": [
    "blob_name = \"images/SuperTrainZip.zip\"\n",
    "blob = bucket.blob(blob_name)\n",
    "\n",
    "source_file_name = \"../Data/Images/SuperTrainZip.zip\" \n",
    "blob.upload_from_filename(source_file_name)\n",
    "\n",
    "print(\"File uploaded to {}.\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List blobs in a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs()\n",
    "\n",
    "print(\"Blobs in {}:\".format(bucket.name))\n",
    "for item in blobs:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T00:05:04.920011Z",
     "start_time": "2021-05-03T00:05:04.916763Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'MAX_LENGTH': max_length,\n",
    "    'ATTENTION_FEATURES_SHAPE': attention_features_shape,\n",
    "    'VOCAB_SIZE': vocab_size,\n",
    "    'UNITS': units,\n",
    "    'EMBEDDING_DIMENSION': embedding_dim,\n",
    "}\n",
    "with open('../Data/Objects/parameters.json', 'w') as json_file:\n",
    "    json.dump(parameters, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T00:02:26.731024Z",
     "start_time": "2021-05-03T00:02:26.704464Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving\n",
    "with open('../Data/Objects/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "u6ghkdDbiDj8"
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Pcp9_n1fiDj8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImageCaptioningDemo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
