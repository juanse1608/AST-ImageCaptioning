{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfJZLpl-iDjs",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Libraries</a></span></li><li><span><a href=\"#Functions-and-Classes\" data-toc-modified-id=\"Functions-and-Classes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Functions and Classes</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Loading-Images-and-Captions\" data-toc-modified-id=\"Loading-Images-and-Captions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loading Images and Captions</a></span></li><li><span><a href=\"#Split:-Train-and-Test\" data-toc-modified-id=\"Split:-Train-and-Test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Split: Train and Test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob7dvTLiLjr8"
   },
   "source": [
    "# Image Captioning - Advanced Statistics Topics\n",
    "\n",
    "In this notebook we adecuate the enviroment to sue the GPU provided by Google Colab. Unfortunately, the storage (CPU) capacity of the vm is too low and we have to read, process, train the model and delete little batches of the training images.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6lCo231DGwz"
   },
   "source": [
    "## Import Libraries\n",
    "\n",
    "In this section we import the requiere libraries and establish the connection with Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.675664Z",
     "start_time": "2021-04-28T15:12:11.016400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1619907527812,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "WdtmqKdzMA0W",
    "outputId": "9f345eca-8bba-43a6-bf7b-e000fd614d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORFLOW VERSION: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Se importan las librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import os\n",
    "import gzip\n",
    "import timeit\n",
    "import shutil\n",
    "import json\n",
    "import collections\n",
    "import zipfile\n",
    "import random\n",
    "import time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    colab = True\n",
    "    from google.colab import drive # Necesario para GoogleColab\n",
    "except:\n",
    "    colab = False\n",
    "\n",
    "# Tensorflow 2.x\n",
    "import tensorflow as tf # Not required for this notebook \n",
    "print('TENSORFLOW VERSION: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.679133Z",
     "start_time": "2021-04-28T15:12:14.677248Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 116602,
     "status": "ok",
     "timestamp": 1619907031720,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "zp8cp8wMMBhN",
    "outputId": "a8b776dc-98a6-4db4-cbca-d48322975d48"
   },
   "outputs": [],
   "source": [
    "# Se fija el directorio referencia con el se conecta colab a drive # Necesario para GoogleColab\n",
    "if colab:\n",
    "    drive.mount('/content/drive', force_remount=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.683150Z",
     "start_time": "2021-04-28T15:12:14.681323Z"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1619907034125,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "b9mNBsu2ENCt"
   },
   "outputs": [],
   "source": [
    "# Se guarda en una variable el directorio del curso\n",
    "\n",
    "# Directorio de los datos\n",
    "data_folder = '../Data/'\n",
    "if colab:\n",
    "    os.chdir('drive/MyDrive/Colab Notebooks/AST-ImageCaptioning/Notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuFdJ_qqiDj0"
   },
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:12:14.687461Z",
     "start_time": "2021-04-28T15:12:14.684804Z"
    },
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1619907038916,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "L2_XEe7jiDj1"
   },
   "outputs": [],
   "source": [
    "# Load an image and adjust it to InceptionV3 networks\n",
    "def image_to_v3_format(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55-8Qt6yiDj2"
   },
   "source": [
    "## Loading Images and Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:56:59.785560Z",
     "start_time": "2021-04-28T16:56:58.829222Z"
    },
    "executionInfo": {
     "elapsed": 3443,
     "status": "ok",
     "timestamp": 1619907057528,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "O1sASNiPiDj2"
   },
   "outputs": [],
   "source": [
    "# Process all the captions\n",
    "annotation_folder = '../Data/Captions/'\n",
    "PATH = '../Data/Images/Train/'\n",
    "with open(annotation_folder + 'captions_train2014.json', 'r') as f:\n",
    "    captions = json.load(f)\n",
    "    \n",
    "# Group all captions together having the same image ID.\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for val in captions['annotations']:\n",
    "    caption = f\"<start> {val['caption']} <end>\"\n",
    "    image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
    "    image_path_to_caption[image_path].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:57:03.291965Z",
     "start_time": "2021-04-28T16:57:03.230269Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1619907155239,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "fIQmx2xOiDj2",
    "outputId": "3e07c078-7ea9-4e0d-a27c-58f0da45b273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE NUMBER OF IMAGES IS 82783 AND THERE IS 414113 CAPTIONS\n"
     ]
    }
   ],
   "source": [
    "image_paths = list(image_path_to_caption.keys())\n",
    "random.shuffle(image_paths)\n",
    "print('THE NUMBER OF IMAGES IS {} AND THERE IS {} CAPTIONS'.format(len(image_paths), len(captions['annotations'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:57:05.503291Z",
     "start_time": "2021-04-28T16:57:05.412503Z"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1619907173810,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "gb8-fvqKiDj3"
   },
   "outputs": [],
   "source": [
    "captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    captions.extend(caption_list)\n",
    "    img_name_vector.extend([image_path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:57:20.361205Z",
     "start_time": "2021-04-28T16:57:09.259177Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8252,
     "status": "ok",
     "timestamp": 1619907187419,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "nEgxXTSPiDj3",
    "outputId": "edb9eab5-b2f8-4392-f0ae-2425bb2ba61c"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T16:59:08.498509Z",
     "start_time": "2021-04-28T16:59:08.224992Z"
    },
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1619907192217,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "HmqPZD0RiDj4"
   },
   "outputs": [],
   "source": [
    "# Get unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# It can be done for big batches because our vm has 128gb of ram\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(2**6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12986,
     "status": "ok",
     "timestamp": 1619907217724,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "AL7GcsaOiDj4"
   },
   "outputs": [],
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "# Choose the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(captions)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Create the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(captions)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "\n",
    "# Calculates the max_length, which is used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3pU7W7HiDj4"
   },
   "source": [
    "## Split: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1619907235256,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "B27QcpzpiDj4"
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    img_name_train.extend([imgt] * capt_len)\n",
    "    cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    img_name_val.extend([imgv] * capv_len)\n",
    "    cap_val.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1619907235256,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "nTZariEQiDj5",
    "outputId": "cb3eab61-dae1-41ab-c057-1064ac483f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF TRAIN IMAGES: 331287 AND NUMBER OF TRAIN CAPTIONS: 331287\n",
      "NUMBER OF TEST IMAGES: 82826 AND NUMBER OF TEST CAPTIONS: 82826\n"
     ]
    }
   ],
   "source": [
    "print('NUMBER OF TRAIN IMAGES: {} AND NUMBER OF TRAIN CAPTIONS: {}'.format(len(img_name_train), len(cap_train)))\n",
    "print('NUMBER OF TEST IMAGES: {} AND NUMBER OF TEST CAPTIONS: {}'.format(len(img_name_val), len(cap_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1619907237580,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "LIIu34E2iDj5"
   },
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = top_k + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1619907300991,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "Xn5OX-QziDj6"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 421,
     "status": "ok",
     "timestamp": 1619907302970,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "dZgitAJBiDj6"
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1619907304637,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "bfGwL57oiDj6"
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1619907307438,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "bZ87pVBjiDj6"
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1619907309948,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "0nOXn9LaiDj7"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1619907428119,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "ESJjGNs-iDj7"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"../Data/TrainCheckpoints/TrainType0\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1619907432702,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "87EYgyAEiDj7"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1619907437084,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "_d1gUN_fiDj7"
   },
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3878750,
     "status": "error",
     "timestamp": 1619913598310,
     "user": {
      "displayName": "Juan Sebastian Corredor Rodriguez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg1WroZFIBN90ho2rIl-Afb94-58pz6AK715YkY=s64",
      "userId": "12217655037166393722"
     },
     "user_tz": 300
    },
    "id": "tPQ-3VroiDj7",
    "outputId": "3af00b66-bb2d-4a05-f3e5-776af15ec7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 7720 Loss 0.6057\r"
     ]
    }
   ],
   "source": [
    "zip_files = os.listdir('../Data/Images/Zips/')\n",
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    \n",
    "    real_batch = 0\n",
    "    for zippy in zip_files:\n",
    "        with zipfile.ZipFile('../Data/Images/Zips/' + zippy, 'r') as zip_ref:\n",
    "            new_temp = '../Data/Images/Temp/'\n",
    "            os.mkdir(new_temp)\n",
    "            zip_ref.extractall(new_temp)\n",
    "\n",
    "        img_train_batch = os.listdir(new_temp + 'Data/Images/Train/')\n",
    "        image_dataset = tf.data.Dataset.from_tensor_slices([new_temp + 'Data/Images/Train/' + i for i in img_train_batch])\n",
    "        image_dataset = image_dataset.map(image_to_v3_format, num_parallel_calls=tf.data.AUTOTUNE).batch(64)\n",
    "        first = True\n",
    "        for img, path in image_dataset:\n",
    "            batch_features = image_features_extract_model(img)\n",
    "            batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "            if first:\n",
    "                batch_features_total = tf.identity(batch_features)\n",
    "                first = False\n",
    "            else:\n",
    "                batch_features_total = tf.concat(axis=0, values=[batch_features_total,  batch_features])\n",
    "            \n",
    "        min_caption = np.inf\n",
    "        cap_train = []\n",
    "        for imgt in img_train_batch:\n",
    "            capt_len = len(img_to_cap_vector['../Data/Images/Train/' + imgt])\n",
    "            if capt_len < min_caption: min_caption = capt_len\n",
    "        if colab: min_caption = 2 # Colab get OOM with the min_caption >= 3\n",
    "        for idx in range(min_caption):\n",
    "            if idx == 0:\n",
    "                batch_train = tf.identity(batch_features_total)\n",
    "            else:   \n",
    "\n",
    "                batch_train = tf.concat(axis=0, values=[batch_train, batch_features_total])\n",
    "                \n",
    "            for imgt in img_train_batch:\n",
    "                cap_train.append(img_to_cap_vector['../Data/Images/Train/' + imgt][idx])\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((batch_train, cap_train))\n",
    "\n",
    "        # Shuffle and batch\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        # Delete the temp folder\n",
    "        shutil.rmtree(new_temp)\n",
    "        \n",
    "        for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss\n",
    "            real_batch += 1\n",
    "            if real_batch % 20 == 0:\n",
    "                average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "                print(f'Epoch {epoch+1} Batch {real_batch} Loss {average_batch_loss:.4f}', end='\\r')\n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(total_loss/num_steps)\n",
    "        \n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}', end='\\n')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n', end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "u6ghkdDbiDj8"
   },
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GpuTrainingType0.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
